\section{机器学习建模}

    \subsection{单层感知机模型}
        \par 感知机（\textbf{perceptron}）是二分类的线性分类模型，属于监督学习算法。输入为实例的特征向量，输出为实例的类别（取 $+1$ 和 $-1$）。感知机旨在求出将输入空间中的实例划分为两类的分离超平面。为求得超平面，感知机导入了基于误分类的损失函数，利用梯度下降法对损失函数进行最优化求解。如果训练数据集是线性可分的，则感知机一定能求得分离超平面。如果是非线性可分的数据，则无法获得超平面。给定输入 \textbf{\textit{x}}，权重 \textbf{\textit{w}} 和偏移 \textbf{\textit{b}}，感知机输出：
        $$o = \sigma(\langle \textbf{\textit{w}}, \textbf{\textit{x}} \rangle + \textbf{\textit{b}}) \qquad \sigma(x) = \begin{cases}
            1 \quad &if \ x > 0 \\ -1 \quad &otherwise
        \end{cases}$$
        \begin{minipage}{\linewidth}
            \begin{center}
                \includegraphics*[width = 0.5\textwidth]{SLP.jpg}
                \captionof*{figure}{Fig.1 Illustation of SLP}
                {\small 图 1 SLP示意图}
            \end{center}
        \end{minipage}
        \vfill
    \subsection{多层感知机模型}
        \par 多层感知机（\textbf{multilayer perceptron，MLP}）是人工神经网络（\textbf{ANN}）的一种。神经网络是对生物神经元的模拟和简化，包括三层：输入层，隐藏层和输出层。输入层用于接受外界向网络内传入的信息，并将这些信息传递给隐藏层。隐藏层中含有隐藏单元，这些单元将从上一层获取的信息进行计算，并将信息传递给输出层。输出层向外界传输已经处理好的信息\cite{ref5}。
        \begin{minipage}{\linewidth}
            \begin{center}
                \includegraphics*[width = 0.6\textwidth]{MLP.jpg}
                \captionof*{figure}{Fig.2 Illustation of MLP \\ 图 2 MLP示意图}
            \end{center}
        \end{minipage}
        \vspace*{1em}
        \par 对于一个多层感知机，第 $l$ 层的输出应为： $$X^l = \sigma(W^{l-1}X^{l-1} + b^{l-1})$$ 其中，$X^l$表示第 $l$ 层的输出结果，$W^{l-1}$ 是第 $l$ 层与第 $l-1$ 层之间的权重矩阵，$b^{l-1}$是第 $l-1$ 层的偏置，$\sigma$ 是非线性激活函数。
        
    \subsection{非线性激活函数}
        \par 激活函数（\textbf{activation function}）通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。如果不使用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。使用激活函数后，能够给神经元引入非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以利用到更多的非线性模型中。
        \par 我们使用的激活函数必须要为非线性激活函数，以单隐藏层模型为例，假设激活函数 $\sigma(\textbf{\textit{x}})=\textbf{\textit{x}}$ 为本身，有 \begin{align*} \textbf{\textit{h}} &= \sigma(\textbf{\textit{W}}_1\textbf{\textit{x}} + \textbf{\textit{b}}_1) \\ o &= \textbf{\textit{w}}_2^T\textbf{\textit{h}} + b_2\end{align*}则输出仍为线性函数，等价于一个单层感知机。因此我们应选用非线性激活函数。常用的非线性激活函数有 \textbf{sigmoid} 函数，\textbf{tanh} 函数或 \textbf{ReLU} 函数等。
        \par \textbf{ReLU} 函数：\textbf{ReLU} 函数实现简单，同时在各种预测任务中表现良好[9]。其提供了一种非常简单的非线性变换。给定变量 $x$，\textbf{ReLU} 函数被定义为该元素与 $0$ 的最大值：$$ReLU(\textbf{\textit{x}}) = \begin{cases}
            x \quad x > 0 \\ 0 \quad x <= 0
        \end{cases}$$
        \par 相较于 \textbf{Sigmoid} 和 \textbf{tanh}，\textbf{ReLU} 有以下优点：
        \par 1) 相比 \textbf{Sigmoid} 和 \textbf{tanh}，\textbf{ReLU} 摒弃了复杂的计算幂指数运算，而是采用了简单的截断，提高了运算速度；
        \par 2) 对于复杂的网络而言，\textbf{Sigmoid} 和 \textbf{tanh} 函数反向传播的过程中，饱和区域非常平缓，接近于 $0$，容易出现梯度消失的问题，减缓收敛速度。\textbf{Relu} 的梯度大多数情况下是常数，有助于解决复杂网络的收敛问题；
        \par 3) \textbf{ReLU} 会使一部分神经元的输出为 $0$，这样使得网络变得更稀疏，并且减少了参数的相互依存关系，缓解了过拟合问题的发生；
        \par 4) \textbf{ReLU} 的另一个优势是在生物上的合理性，它是单边的，相比 \textbf{sigmoid} 和 \textbf{tanh}，更符合生物神经元的特征。\textbf{ReLU} 更容易学习优化。因为其分段线性性质，导致其前传，后传，求导都是分段线性。而传统的 \textbf{Sigmoid} 函数，由于两端饱和，在传播过程中容易丢弃信息\cite{ref9}。
        \par 基于阅读相关文献获得这些原因，我们选取 \textbf{ReLU} 作为我们的激活函数。
    
    \subsection{损失函数}
        \par 损失函数（\textbf{loss function}）用来度量模型的预测值 $f(x)$ 与真实值 $Y$ 的差异程度的运算函数，它是一个非负实值函数，通常使用 $L(Y, f(x))$来表示，损失函数越小，模型的鲁棒性就越好。
        \par 均方误差（\textbf{mean square error,MSE}）是回归损失函数中最常用的误差，它是预测值 $f(x)$ 与目标值 $Y$ 之间差值平方和的均值，其公式如下： $$MSE = \frac{1}{n}\sum_{i=1}^{n}(f(x) - Y)^2$$
        \par 对于数据库基数估计任务，由于基数可能会很大，所以采用对数形式进行运算，公式变为：\begin{align*} MSE &= \frac{1}{n}\sum_{i=1}^{n}[log(act\_rows) - log(est\_rows)] \\ &= \frac{1}{n}\sum_{i=1}^{n}log(\frac{act\_rows}{est\_rows}) = MSE(q\_error)\end{align*}
        \par 其中，$est\_rows=f(x), act\_rows=Y$, $q\_error = max(\frac{act\_rows}{est\_rows}, \frac{est\_rows}{act\_rows})$
        
    \subsection{反向传播}
        \par 反向传播（\textbf{backpropagation，BP}）是对多层人工神经网络进行梯度下降的算法，即用链式法则以网络每层的权重为变量计算损失函数的梯度，以更新权重来最小化损失函数。
        \par 该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。 假设我们有函数 $Y=f(X)$ 和 $Z=g(Y)$，其中输入和输出$X$, $Y$, $Z$是任意形状的张量。 利用链式法则，我们可以计算 $Z$ 关于 $X$ 的导数\cite{ref10} $$\frac{\partial Z}{\partial X} = prod(\frac{\partial Z}{\partial Y}, \frac{\partial Y}{\partial X})$$
        \par 多层感知机的训练通常使用反向传播算法来更新权重和偏置，以最小化预测结果与真实标签之间的误差。反向传播算法通过计算误差的梯度来调整每个神经元的权重和偏置，从而逐步优化网络的性能。在训练过程中，可以使用不同的优化器进一步改善网络的训练效果。

    \subsection{优化器}
        \par 常见的优化器有 \textbf{Adam}、\textbf{SGD}、\textbf{Momentum} 等，本文中选取了前两者作为模型的优化器。 \\
        \textbf{Adam} 优化器：
        \par 自适应矩估计（\textbf{adaptive moment estimation，Adam}）\cite{ref11}是一种自适应学习率的算法，对每一个参数都计算自适应学习率。其在存储一个指数衰减的历史平方梯度的平均的同时，还保存一个历史梯度的指数衰减均值 $m_t$，类似于动量。 \begin{align*}m_t &= \beta_1m_{t-1} + (1-\beta_1)g_t \\ v_t &= \beta_2v_{t-1} + (1-\beta_2)g_t^2 \end{align*}
        \par 其中，$m_t$ 和 $v_t$ 分别是对梯度一阶矩和二阶矩的估计。而当 $m_t$ 和 $v_t$ 初始化为 $0$ 向量，尤其当衰减率很小时，它们都偏向于 $0$。这可以通过计算偏差校正后的 $m_t$ 和 $v_t$ 的来抵消\cite{ref12} \begin{align*} \hat{m_t} &= \frac{m_t}{1-\beta_1^t} \\ \hat{v_t} &= \frac{v_t}{1-\beta_2^t}\end{align*}
        \par 利用上述的公式更新参数，由此生成了 \textbf{Adam} 的更新公式。 $$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v_t}}+\varepsilon}\hat{m_t}$$ \\
        \textbf{SGD} 优化器：
        \par 随机梯度下降 （\textbf{stochastic gradient descent, SGD}）\cite{ref12}是一种常见的优化器，也是机器学习模型中最基础的优化算法之一。它是梯度下降算法（\textbf{GD}）的一种实现方式，常被用于神经网络中的权重更新（文献引用）。
        \par \textbf{SGD} 优化算法在每次参数更新时，仅选取一个样本 $x^{(i)}, y^{(i)}$ 计算其梯度，参数更新公式为 $$\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta J_i(\theta; x^{(i)}; y^{(i)})$$
        \par \textbf{SGD} 通过一次迭代进行一次更新消除了在处理大型数据集时的冗余计算，训练速度快，但由于每次迭代并不是向整体最优化的方向，\textbf{SGD} 更容易得到一个局部最优解，导致准确度下降。
        \par \textbf{Adam}和 \textbf{SGD} 作为机器学习领域中常用的两种优化算法，各自具有一些独特的特性。\textbf{Adam} 算法通过一阶和二阶梯度矩估计，以及自适应学习率，可以在数据量大或者参数复杂的场景下快速收敛。相比之下，\textbf{SGD}算法虽然简单，但在数据量小或者对精度要求高的场景下，仍然能够找到更平坦的最小值，从而提高泛化能力。为更好的提升模型性能，我们在实际训练中分别测试了这两种优化器对模型性能的影响。

    \subsection{三种特征值提取方法\cite{ref13}}
        \par 1) \textbf{AVI}：方法假设不同查询条件的选择率 (\textbf{selectivity}) 之间相互独立，即满足乘法原则。其中，选择率代表某个查询查询出的记录行数占总行数的百分比。在这个假设下，总选择率为 $\\Pi_{k=1}^{d}(s_k)$。通过将总的选择率乘上数据总行数，便得出 \textbf{AVI} 方法估计出来的基数
        \par 2) \textbf{EBO}：此方法不同于 \textbf{AVI} 方法考虑所有的列，其认为应选出 $4$ 个最小的选择率组成总的选择率，为 $s_{(1)}\times s_{(2)}^{0.5} \times s_{(3)}^{0.25} \times s_{(4)}^{0.125}$，其中，$s_{(k)}$ 代表第 $k$ 小的选择率。通过将总的选择率乘上数据总行数，便得出 \textbf{EBO} 方法估计出来的基数
        \par 3) \textbf{minimumMinSel}：该方法较为简单，其认为总的选择率便是最小的选择率。通过将总的选择率乘上数据总行数，便得出 \textbf{MinSel} 方法估计出来的基数



    
